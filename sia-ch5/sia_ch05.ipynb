{"cells":[{"cell_type":"code","source":["dbutils.fs.ls('/FileStore/tables/pc5ly1131502251351157/italianPosts.csv')"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["#### italianPosts.csv\n1. commentCount—Number of comments related to the question/answer\n1. lastActivityDate—Date and time of the last modification\n1. ownerUserId—User ID of the owner\n1. body—Textual contents of the question/answer\n1. score—Total score based on upvotes and downvotes\n1. creationDate—Date and time of creation\n1. viewCount—View count\n1. title—Title of the question\n1. tags—Set of tags the question has been marked with\n1. answerCount—Number of related answers\n1. acceptedAnswerId—If a question contains the ID of its accepted answer\n1. postTypeId—Type of the post; 1 is for questions, 2 for answers\n1. id—Post’s unique ID"],"metadata":{}},{"cell_type":"code","source":["# load into rdd\nrddItalianPosts_1 = sc.textFile('/FileStore/tables/pc5ly1131502251351157/italianPosts.csv') \\\n                    .map(lambda line: line.split('~'))\nrddItalianPosts_1.count()\nrddItalianPosts_1.take(1)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["#### 5.1.1\n#### Creating a DataFrame from an RDD of tuples"],"metadata":{}},{"cell_type":"code","source":["# convert each RDD array to tuple\nrddItalianPosts_2 = rddItalianPosts_1.map(lambda l: (l[0], l[1], l[2], l[3], l[4], l[5], l[6], l[7], l[8], l[9], l[10], l[11], l[12]))\nrddItalianPosts_2.take(1)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# now convert to DF\ndfItalianPosts = rddItalianPosts_2.toDF(['commentCount', 'lastActivityDate', 'ownerUserId', 'body', 'score', 'creationDate', 'viewCount', 'title', 'tags', 'answerCount', 'acceptedAnswerId', 'postTypeId', 'id'])\ndfItalianPosts.show(2)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["##### Note that the field data type are incorrect"],"metadata":{}},{"cell_type":"code","source":["dfItalianPosts.printSchema()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["We'll need to define schema manually to set the proper datatypes"],"metadata":{}},{"cell_type":"markdown","source":["#### Converting RDDs to DataFrames by specifying a schema"],"metadata":{}},{"cell_type":"code","source":["from datetime import datetime\n\ndef toIntSafe(val):\n  try:\n    return int(val)\n  except ValueError:\n    return None\n  \ndef toLongSafe(val):\n  try:\n    return long(val)\n  except ValueError:\n    return None\n  \ndef toTimeSafe(val):\n  try:\n    return datetime.strptime(val, \"%Y-%m-%d %H:%M:%S.%f\")\n  except ValueError:\n    return None\n  \n# Method to convert string to a Row\nfrom pyspark.sql import Row\ndef stringToPost(string):\n  l_s = string.encode('utf8').strip().split('~')\n  return Row(\n    toIntSafe(l_s[0]),   # commentCount\n    toTimeSafe(l_s[1]),  # lastActivityDate\n    toLongSafe(l_s[2]),  # ownerUserId\n    l_s[3],              # body\n    toIntSafe(l_s[4]),   # score\n    toTimeSafe(l_s[5]),  # creationDate\n    toIntSafe(l_s[6]),   # viewCount\n    l_s[7],              # title\n    l_s[8],              # tags\n    toIntSafe(l_s[9]),   # answerCount\n    toLongSafe(l_s[10]), # acceptedAnswerId\n    toLongSafe(l_s[11]), # postTypeId\n    toLongSafe(l_s[12]), # id\n  )\n  \n# Define schema\nfrom pyspark.sql.types import *\npostSchema = StructType([\n    StructField('commentCount', IntegerType(), True),\n    StructField('lastActivityDate', TimestampType(), True),\n    StructField('ownerUserId', LongType(), True),\n    StructField('body', StringType(), True),\n    StructField('score', IntegerType(), True),\n    StructField('creationDate', TimestampType(), True),\n    StructField('viewCount', IntegerType(), True),\n    StructField('title', StringType(), True),\n    StructField('tags', StringType(), True),\n    StructField('answerCount', IntegerType(), True),\n    StructField('acceptedAnswerId', LongType(), True),\n    StructField('postTypeId', LongType(), True),\n    StructField('id', LongType(), False),\n  ])"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Now, create RDD using schema\nrddItalianPosts_3 = sc.textFile('/FileStore/tables/pc5ly1131502251351157/italianPosts.csv') \\\n                      .map(lambda line: stringToPost(line))\nrddItalianPosts_3.count()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["dfItalianPosts_1 = sqlContext.createDataFrame(rddItalianPosts_3, schema=postSchema)\ndfItalianPosts_1.take(1)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["dfItalianPosts_1.printSchema()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["dfItalianPosts_1.columns"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["dfItalianPosts_1.dtypes"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### 5.1.2 API basics"],"metadata":{}},{"cell_type":"markdown","source":["##### [select](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select)"],"metadata":{}},{"cell_type":"code","source":["# 1. select can take column names\ndfPostsIdBody = dfItalianPosts_1.select('id', 'body')\ndfPostsIdBody.show(1)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# 2. select can take Column type\ndfPostsIdBody = dfItalianPosts_1.select(dfItalianPosts_1.id, dfItalianPosts_1.body) # this is the most used methon in spark doc\ndfPostsIdBody = dfItalianPosts_1.select(dfItalianPosts_1['id'], dfItalianPosts_1['body'])\n\nfrom pyspark.sql.functions import col\ndfPostsIdBody = dfItalianPosts_1.select(col('id'), col('body'))\ndfPostsIdBody.show(1)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["dfPostsExceptBody = dfItalianPosts_1.select('*').drop(dfItalianPosts_1.body)\ndfPostsExceptBody.show(1)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# where() is an alias of filter()\n# dfPostsIdBody.filter('Italiano' in dfPostsIdBody.body).count()\nfrom pyspark.sql.functions import instr\ndfPostsIdBody.filter(instr(dfPostsIdBody.body, 'Italiano') > 0).count()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# NOTE how '&' is used for 'and', parenthesis around each sub expression \ndfPostsExceptBody.filter((dfPostsExceptBody.postTypeId == 1) & (dfPostsExceptBody.acceptedAnswerId.isNull())).count()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# limit the result set\ndfPostsExceptBody.filter((dfPostsExceptBody.postTypeId == 1) & (dfPostsExceptBody.acceptedAnswerId.isNull())).limit(10).show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["dfPostsExceptBody.withColumnRenamed('ownerUserId', 'ownerId').show(1)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["dfPostsExceptBody.withColumn('ratio', dfPostsExceptBody.viewCount / dfPostsExceptBody.score).filter('ratio < 35').count()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# The 10 most recently modified questions\ndfPostsExceptBody.orderBy(dfPostsExceptBody.lastActivityDate.desc()).limit(10).show()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["dfPostsExceptBody.orderBy(dfPostsExceptBody.lastActivityDate, ascending=False).limit(10).show()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["dfPostsExceptBody.orderBy('lastActivityDate', ascending=False).limit(10).show()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from pyspark.sql.functions import desc\ndfPostsExceptBody.orderBy(desc('lastActivityDate')).limit(10).show()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# multiple sort orders\ndfPostsExceptBody.orderBy([dfPostsExceptBody.lastActivityDate, dfPostsExceptBody.creationDate], ascending=[False, True]).limit(10).show()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### 5.1.3 SQL Functions\n\nSQL functions fit into four categories:\n* Scalar functions return a single value for each row based on calculations on one or more columns.\n* Aggregate functions return a single value for a group of rows.\n* Window functions return several values for a group of rows.\n* User-defined functions include custom scalar or aggregate functions."],"metadata":{}},{"cell_type":"code","source":["# find the question that was active for the largest amount of time\nfrom pyspark.sql.functions import *\ndfItalianPosts_1 \\\n  .filter(dfItalianPosts_1.postTypeId == 1) \\\n  .withColumn('activePeriod', datediff(dfItalianPosts_1.lastActivityDate, dfItalianPosts_1.creationDate)) \\\n  .orderBy(desc('activePeriod')) \\\n  .head().body.replace('&lt;', '<').replace('&gt;', '>')"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# find the average and maximum score of all questions and the total number of questions\ndfItalianPosts_1.select(avg(dfItalianPosts_1.score), max(dfItalianPosts_1.score), count(dfItalianPosts_1.score)).show()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["## First, you’ll display the maximum score of all user questions (post typeID 1),\n## and for each question, how much its score is below the maximum score for that user.\n\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import *\ndfIP_1 = dfItalianPosts_1 \\\n  .filter(dfItalianPosts_1.postTypeId == 1) \\\n  .select(dfItalianPosts_1.ownerUserId, dfItalianPosts_1.acceptedAnswerId, dfItalianPosts_1.score, max(dfItalianPosts_1.score).over(Window.partitionBy(dfItalianPosts_1.ownerUserId)).alias('maxPerUser'))\ndfIP_1.show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["dfIP_2 = dfIP_1.withColumn('toMax', dfIP_1.maxPerUser - dfIP_1.score)\ndfIP_2.show()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# For each question, display the id of its owner’s next and previous questions by creation date\n\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import *\ndfIP_1 = dfItalianPosts_1 \\\n  .filter(dfItalianPosts_1.postTypeId == 1) \\\n  .select(dfItalianPosts_1.ownerUserId, dfItalianPosts_1.id, dfItalianPosts_1.creationDate,\\\n          lag(dfItalianPosts_1.id, count=1).over(Window.partitionBy(dfItalianPosts_1.ownerUserId).orderBy(dfItalianPosts_1.creationDate)).alias('prev'),\\\n          lead(dfItalianPosts_1.id, count=1).over(Window.partitionBy(dfItalianPosts_1.ownerUserId).orderBy(dfItalianPosts_1.creationDate)).alias('next')) \\\n  .orderBy([dfIP_1.ownerUserId, dfItalianPosts_1.id])\ndfIP_1.show()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["#### UDF: User-defined functions\nUDF's let us extend build-in sql.functions"],"metadata":{}},{"cell_type":"code","source":["# 1. Use udf() to create a UDF\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import udf\ncountTags = udf(lambda tags: tags.count('&lt;'), IntegerType())\ndfIP_1 = dfItalianPosts_1 \\\n  .filter(dfItalianPosts_1.postTypeId == 1) \\\n  .select(dfItalianPosts_1.tags, countTags(dfItalianPosts_1.tags).alias('tagCount'))\ndfIP_1.show(10, False)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# 2. User SparkSession udf.register to register UDF\n# This also allows us to use UDF in SQL expressions\nfrom pyspark.sql.types import StringType\n\nsqlContext.udf.register('replaceAngleBracketsUDF', lambda tags: tags.replace('&lt;', '<').replace('&gt;', '>'), StringType())\n\ndfIP_1 = dfItalianPosts_1 \\\n  .filter(dfItalianPosts_1.postTypeId == 1) \\\n  .selectExpr('replaceAngleBracketsUDF(tags)')\ndfIP_1.show(10, False)\n"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["### 5.1.4"],"metadata":{}},{"cell_type":"code","source":["# na returns DataFrameNaFunctions\ndfClean = dfItalianPosts_1.na.drop() # drop rows if any column has null\ndfClean.show()\ndfClean.count()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["dfItalianPosts_1.na.drop(subset=['acceptedAnswerId']).show()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["dfItalianPosts_1.na.fill({'viewCount': 0}).show()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["dfItalianPosts_1.na.replace(1177, 3000, ['id', 'acceptedAnswerId']).show()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["### 5.1.5 Converting DataFrame to RDD"],"metadata":{}},{"cell_type":"markdown","source":["Typically there’s no need to convert DataFrames to RDDs and back because most\ndata-mapping tasks can be done with built-in DSL and SQL functions and UDFs."],"metadata":{}},{"cell_type":"code","source":["def replaceLtGt(row):\n  return Row(\n    commentCount=row.commentCount,\n    lastActivityDate=row.lastActivityDate,\n    ownerUserId=row.ownerUserId,\n    body=row.body.replace('&lt;', '<').replace('&gt;', '>'),\n    score=row.score,\n    creationDate=row.creationDate,\n    viewCount=row.viewCount,\n    title=row.title,\n    tags=row.tags.replace('&lt;', '<').replace('&gt;', '>'),\n    answerCount=row.answerCount,\n    acceptedAnswerId=row.acceptedAnswerId,\n    postTypeId=row.postTypeId,\n    id=row.id\n  )\nrddPosts = dfItalianPosts_1.rdd.map(replaceLtGt)\nrddPosts.take(1)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["# RDD to DF with reordering columns\ndef sortSchema(schema):\n  fields = {f.name: f for f in schema.fields}\n  names = sorted(fields.keys())\n  return StructType([fields[name] for name in names])\n\ndfItalianPosts_mapped = sqlContext.createDataFrame(rddPosts, schema=sortSchema(dfItalianPosts_1.schema))\ndfItalianPosts_mapped.show(5)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["### 5.1.6. Grouping and joining data\nDataFrame.groupBy() returns GroupedData()"],"metadata":{}},{"cell_type":"code","source":["# To find the number of posts per author, associated tags, and the post type\ndfPostsPerAuthor = dfItalianPosts_mapped.groupBy(['ownerUserId', 'tags', 'postTypeId']).count()\ndfPostsPerAuthor.orderBy(dfItalianPosts_mapped.ownerUserId.desc()).show(5)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["# find the last activity date and the maximum post score per user\nfrom pyspark.sql.functions import *\ndfItalianPosts_mapped.groupBy(dfItalianPosts_mapped.ownerUserId) \\\n  .agg(max(dfItalianPosts_mapped.lastActivityDate), max(dfItalianPosts_mapped.score)) \\\n  .withColumnRenamed('max(lastActivityDate)', 'maxLastActivityDate') \\\n  .withColumnRenamed('max(score)', 'maxScore') \\\n  .orderBy(dfItalianPosts_mapped.ownerUserId) \\\n  .show(5)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["dfItalianPosts_mapped.groupBy(dfItalianPosts_mapped.ownerUserId) \\\n  .agg({'lastActivityDate': 'max', 'score': 'max'}) \\\n  .orderBy('ownerUserId') \\\n  .show(5)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["dfItalianPosts_mapped.groupBy(dfItalianPosts_mapped.ownerUserId) \\\n  .agg(max(dfItalianPosts_mapped.lastActivityDate), max(dfItalianPosts_mapped.score) > 5) \\\n  .orderBy('ownerUserId') \\\n  .show(5)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["# rollup\ndfSample = dfItalianPosts_mapped.where((dfItalianPosts_mapped.ownerUserId >= 13) & (dfItalianPosts_mapped.ownerUserId <= 15))\ndfSample.count()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["dfSample.show()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["# Counting the posts by owner, tags, and post type\ndfSample.groupBy(dfSample.ownerUserId, dfSample.tags, dfSample.postTypeId).count().show()"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["dfSample.rollup(dfSample.ownerUserId, dfSample.tags, dfSample.postTypeId).count().show()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["dfSample.cube(dfSample.ownerUserId, dfSample.tags, dfSample.postTypeId).count().show()"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["#### Configuring Spark SQL\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.conf"],"metadata":{}},{"cell_type":"code","source":["# SparkSession\nspark.conf.set(\"SET spark.sql.caseSensitive=true\")\nspark.conf.set(\"spark.sql.caseSensitive\", \"true\")"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["sqlContext.sql(\"SET spark.sql.caseSensitive=true\")\nsqlContext.setConf(\"spark.sql.caseSensitive\", \"true\")"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["spark.conf.get('spark.sql.caseSensitive')"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["spark.conf.get('spark.sql.tungsten.enabled')"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["### 5.1.7 Performing Joins"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.ls('/FileStore/tables/buj2ar781502756511985/italianVotes.csv')"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["def italianVoteToRow(line):\n  l_vote = line.encode('utf8').split('~')\n  return Row(\n    id=long(l_vote[0]),\n    postId=long(l_vote[1]),\n    voteTypeId=int(l_vote[2]),\n    creationDate=datetime.strptime(l_vote[3], \"%Y-%m-%d %H:%M:%S.%f\") \n  )\n\nrddItalianVotes = sc.textFile('/FileStore/tables/buj2ar781502756511985/italianVotes.csv').map(italianVoteToRow)\nrddItalianVotes.take(5)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["voteSchema = StructType([\n    StructField('creationDate', TimestampType(), False),\n    StructField('id', LongType(), False),\n    StructField('postId', LongType(), False),\n    StructField('voteTypeId', IntegerType(), False),\n  ])\n\ndfItalianVotes = sqlContext.createDataFrame(rddItalianVotes, schema=voteSchema)\ndfItalianVotes.show(5)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join"],"metadata":{}},{"cell_type":"code","source":["sqlContext.setConf('spark.sql.shuffle.partitions', 5)\ndfItalianPosts_mapped.join(dfItalianVotes, dfItalianPosts_mapped.id == dfItalianVotes.postId, 'inner').show(5)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["#### DataSets\nThe idea behind DataSets “is to provide an API that allows users to easily express transformations on domain objects, while\nalso providing the performance and robustness advantages of the Spark SQL execution engine”.\nThat essentially means that you can store ordinary Java objects in DataSets and take advantage of Tungsten and Catalyst optimizations.\n\nDataFrames are now simply implemented as DataSets containing Row objects."],"metadata":{}},{"cell_type":"markdown","source":["### 5.3.1"],"metadata":{}},{"cell_type":"code","source":["# Register temp table\ndfItalianPosts_mapped.createOrReplaceTempView('posts_temp')"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["# Register table permanently\ndfItalianVotes.write.saveAsTable('votes')"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["# View table cataloges\nspark.catalog.listTables()"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["spark.catalog.listColumns('votes')"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["spark.catalog.listFunctions()"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":["### 5.3.2 Executing SQL queries"],"metadata":{}},{"cell_type":"code","source":["dfPostsTemp = sqlContext.sql('SELECT * FROM posts_temp')\ndfPostsTemp.show(5)"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["dfPostsTemp.printSchema()"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["%sql\nselect * from votes limit(10);"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["%sql\nselect substring(title, 0, 70) from posts_temp where postTypeId = 1 order by creationDate desc limit 3;"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":83}],"metadata":{"name":"sia_ch05","notebookId":46921479939159},"nbformat":4,"nbformat_minor":0}
